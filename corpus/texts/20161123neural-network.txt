
Разработчики из Оксфордского университета и лаборатории Google DeepMind создали систему искусственного интеллекта, которая может распознавать речь по губам в реальных условиях, причем делает она это лучше человека. Для ее обучения исследователи использовали фрагменты телепередач BBC. С текстом статьи можно ознакомиться на сайте ArXiv. 
Автоматические системы распознавания речи на основе мимики человека могут использоваться самыми различными способами: например, для создания слуховых аппаратов нового поколения, биометрической идентификации или расследования преступлений. Поэтому ученые уже много лет работают над разработкой программ для «чтения» по губам, однако их успех в этой области весьма ограничен. Только в этом месяце исследователи представили первую в мире систему LipNet, которая может распознавать речь на уровне целых предложений лучше человека. Однако даже в ней имелись изъяны. Главным недостатком LipNet было то, что для проверки ее работы использовалась база данных с ограниченным числом дикторов, которые говорили предложения, построенные по одинаковому принципу. Такие условия признали «тепличными» даже сами разработчики.
Авторы новой работы представили систему Watch, Listen, Attend and Spell (WLAS), которая распознает речь реальных телеведущих программ BBC. Она построена по тому же принципу, что и LipNet: в ее основе лежит сочетание использования LSTM и сверточных нейросетей и методов машинного обучения. LSTM-нейросети представляют собой разновидность рекуррентных нейросетей, для которых характерно наличие обратной связи. Их главная особенность состоит в том, что они способны обучаться долговременным зависимостям и, как следствие, работать с контекстом в длинных предложениях (подробнее о LSTM и рекуррентных нейросетях вы можете прочитать в нашем материале). Сверточные нейросети, в свою очередь, хорошо справляются с задачей распознавания изображений и подходят для покадрового анализа видео.
Система WLAS обучалась с помощью базы данных, состоявшей из 5 тысяч часов записей телепередач BBC. В общей сложности она содержала 118 тысяч предложений, которые произносились разными людьми. Сначала искусственный интеллект учился распознавать «по губам» отдельные слова, сопоставляя движения губ говорящего с субтитрами, а после переходил на уровень словосочетаний и отдельных предложений. Кроме того, WLAS была дополнительно обучена распознавать речь по аудиодорожкам.
Около 12 тысяч предложений из общей базы данных были использованы для проверки новой системы. Тесты показали, что в 46,8 процентах случаев WLAS правильно определяет, что сказал участник телепрограммы. При этом многие ошибки были незначительными — например, программа пропускала «s» в конце слов. Таким образом искусственному интеллекту удалось обойти человека: точность специально обученных людей, которые расшифровывали 200 случайно выбранных видео из той же базы данных, достигала всего 12,4 процентов (если из оценки исключались те предложения, с которыми расшифровщик отказывался работать, то она возрастала до 26,2). 
С таким результатом WLAS обходит все существующие системы распознавания речи по мимике человека. Тем не менее, пока не ясно, где именно будет использована программа. Авторы работы заявляют, что с ее помощью можно будет улучшить работу таких мобильных ассистентов, как Siri.
Недавно компания Microsoft объявила о том, что ей удалось усовершенствовать систему распознавания устной речи, работа которой также основана на использовании сверточных и LSTM-нейросетей. Теперь программа, которую планируется использовать в голосовом помощнике Cortana, игровой приставке Xbox One и других программах, делает меньше ошибок, чем профессиональный специалист по набору текста.
Кристина Уласович
