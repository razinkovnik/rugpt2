
Разработчики из Массачусетского технологического института создали программу, превращающую фотографию в короткое видео, которое показывает, как будут развиваться события на снимке в ближайшую секунду. Для этого они использовали нейросети и глубинное обучение. Работа исследователей будет представлена 5 декабря на конференции в Барселоне.
Современные нейросети хорошо справляются с задачей распознавания изображений. Например, для них не составляет труда узнать человека, если его лицо размыто или закрашено, а также угадать, что именно хотел изобразить пользователь, даже если его рисунки напоминают каракули. Однако когда речь заходит о том, чтобы по фотографии определить что может произойти в ближайшем будущем, у искусственного интеллекта возникают сложности. Например, человек легко догадается, что если на снимке показан мужчина, который кладет в свою тарелку салат, то скорее всего в дальнейшем он будет его есть, однако нейросети сделать такое простое предположение будет трудно. В то же время, искусственному интеллекту необходимо уметь «предвидеть» будущее для того, чтобы лучше понимать настоящее: например, это поможет системам управления беспилотных автомобилей оценивать вероятность возникновения аварии. Поэтому авторы новой работы создали программу, которая по фотографии может предположить, что произойдет в следующий момент, и сделать на основе этого видео.
Исследователи использовали порождающую состязательную модель (generative adversarial network), в которой две нейросети — генеративная и различающая — «воюют» друг с другом. Принцип ее работы довольно прост: генеративная нейросеть старается обмануть различающую нейросеть, создавая такие образцы (в данном случае видео), которые ее «соперница» не сможет отличить от некоторых настоящих, эталонных образцов (реальных видео). В результате получается нечто вроде соревнования между двумя системами: одна учится делать качественные «подделки», а другая — их находить, что позволяет добиться хорошего конечного результата.
В качестве исходного материала для обучения нейросети разработчики использовали два миллиона видео с Flickr. На них были показаны четыре типа сцен — прибытие поезда на вокзал, занятия по гольфу, пляж и младенцы в больничной палате. Все видеозаписи не содержали меток, то есть в них не было подсказок, которые помогли бы искусственному интеллекту понять, что именно он видит. С помощью этих видео искусственный интеллект учился понимать, какие события типичны для разных категорий сцен. После этого исследователи давали системе статичный кадр и заставляли превращать его в видео на основе проанализированных данных. И вот здесь как раз и возникало соревнование между генеративной и различающей нейросетью.
